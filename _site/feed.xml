<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://pinkr1ver.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://pinkr1ver.com/" rel="alternate" type="text/html" /><updated>2022-12-17T15:48:38+08:00</updated><id>https://pinkr1ver.com/feed.xml</id><title type="html">PinkR1ver Studio</title><subtitle>Jude Wang's Blog
</subtitle><author><name>{&quot;fullname&quot;=&gt;&quot;Jude Wang&quot;, &quot;github&quot;=&gt;&quot;PinkR1ver&quot;, &quot;email&quot;=&gt;&quot;pinkr1veroops@gmail.com&quot;, &quot;twitter&quot;=&gt;&quot;PinkR1ver&quot;, &quot;instagram&quot;=&gt;&quot;Jude.wang.YC&quot;}</name><email>pinkr1veroops@gmail.com</email></author><entry><title type="html">Review:A Generalized Sampling Method for Finite-Rate-of-Innovation-Signal Reconstruction</title><link href="https://pinkr1ver.com/paper-review/2022/07/22/review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction.html" rel="alternate" type="text/html" title="Review:A Generalized Sampling Method for Finite-Rate-of-Innovation-Signal Reconstruction" /><published>2022-07-22T23:36:00+08:00</published><updated>2022-07-22T23:36:00+08:00</updated><id>https://pinkr1ver.com/paper-review/2022/07/22/review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction</id><content type="html" xml:base="https://pinkr1ver.com/paper-review/2022/07/22/review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction.html"><![CDATA[<h2 id="article-address">Article Address:</h2>
<p><a href="https://ieeexplore.ieee.org/document/4682542">https://ieeexplore.ieee.org/document/4682542</a></p>

<h2 id="background">Background</h2>
<p>é‡‡æ ·å’Œé‡æ„æ— æ³•åœ¨é¦™å†œé‡‡æ ·ç†è®ºçš„ç»å…¸æ¡†æ¶å†…å¤„ç†çš„ä¿¡å·,æ­¤ç±»ä¿¡å·çš„ç¤ºä¾‹åŒ…æ‹¬ç‹„æ‹‰å…‹è„‰å†²æµå’Œå„ç§ç±»å‹çš„éå‡åŒ€æ ·æ¡ï¼Œä¾‹å¦‚åˆ†æ®µå¤šé¡¹å¼ã€åˆ†æ®µæŒ‡æ•°å’Œåˆ†æ®µè°æ³¢å‡½æ•°ã€‚è¿™äº›ä¿¡å·ç”±ä¸€ç»„ç¦»æ•£çš„å‚æ•°ï¼ˆä¾‹å¦‚ï¼Œå¥‡ç‚¹çš„å¹…åº¦å’Œä½ç½®ï¼‰å”¯ä¸€æŒ‡å®šï¼Œå› æ­¤å…·æœ‰å…·æœ‰æœ‰é™æ–°ç‡ï¼ˆFRIï¼‰</p>

<h2 id="from-dirac-impulse-to-kronecker-impulse">From Dirac Impulse to Kronecker Impulse</h2>
<p><img src="/assets/img/2022-07-22-review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction/fig2.png" alt="From Dirac impulses to Kronecker impulses" /></p>

<p>éœ€è¦ç†è§£çš„åœ°æ–¹å°±åœ¨äº(4)çš„\(p_a(nT)\),å®ƒæ˜¯ç”±ä¸€ä¸ªå«åšdiscrete-time finite-impulse-response
filter specified by the Z-transform:\(G_a(z)=\frac{1}{a}(1-e^{-aT}z^{-T})\)çš„filterå¤„ç†åå¾—åˆ°çš„ã€‚å½“ (3) ä¸­çš„åºåˆ—ç”± G(z) å¤„ç†æ—¶ï¼Œå®ƒä¼šäº§ç”Ÿå…‹ç½—å†…å…‹è„‰å†²æµï¼Œä¹Ÿå°±æ˜¯(4)</p>

<p>Kronecker Impulseçš„å¹…åº¦æ˜¯Dirac Impulseçš„å¹…åº¦å’Œä½ç½®çš„å¯åˆ†å‡½æ•°</p>

<h2 id="two-channel-sampling-of-a-dirac-impulse-train">Two Channel sampling of a dirac impulse train</h2>

<p>Kronecker Impulseçš„å¹…åº¦æ˜¯Dirac Impulseçš„å¹…åº¦å’Œä½ç½®çš„å¯åˆ†å‡½æ•°ï¼Œè¿™ä¸€ç‰¹æ€§å¯ä»¥ç»™æˆ‘ä»¬ä¸€ç§ç®€å•è€Œæœ‰æ•ˆçš„é‡å»ºæŠ€æœ¯ã€‚</p>

<p>å…·ä½“ç»†èŠ‚ä¸Šï¼Œæˆ‘ä»¬é€šè¿‡ä¸åŒçš„é‡‡æ ·å†…æ ¸è·å¾—å¦ä¸€ç»„æµ‹é‡å€¼ï¼Œå¦‚ä¸‹å›¾ï¼š
<img src="/assets/img/2022-07-22-review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction/fig3.png" alt="Two-channel sampling of a stream of Dirac impulses by using first-order RC networks." /></p>

<p>ç„¶åå¯ä»¥é€šè¿‡\(p_a(nT)\)å’Œ\(p_\gamma(nT)\)å»ºç«‹ç­‰å¼è®¡ç®—åŸä¿¡å·ï¼Œå…·ä½“è§è®ºæ–‡</p>

<h2 id="spline-equivalence-and-effects">Spline Equivalence and Effects</h2>
<p>å¯¹äºEæ ·æ¡(E-spline )é‡‡æ ·ï¼ˆä¸ç¡®å®šSpline Equivalenceçš„å‡†ç¡®å®šä¹‰ï¼‰ä¸Diracç›¸ä¼¼ï¼Œé€šè¿‡å¤šé€šé“é‡‡æ ·å’Œä¸åŒçš„é‡‡æ ·æ ¸é‡å»ºä¿¡å·ï¼Œæ•ˆæœå¦‚ä¸‹ï¼š</p>

<p><img src="/assets/img/2022-07-22-review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction/fig4.png" alt="Ground truth and Reconstruction" /></p>

<h2 id="conclusion">Conclusion</h2>
<p>æ€»è€Œè¨€ä¹‹ï¼Œæœ¬ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åŒé€šé“é‡‡æ ·æ–¹æ³•æ¥ä½¿ç”¨ç®€å•çš„è®¡ç®—æ¥æ£€ç´¢è„‰å†²çš„å‚æ•°æ¥è§£å†³äº†é‡‡æ ·å’Œé‡å»ºç‹„æ‹‰å…‹è„‰å†²æµçš„å…¸å‹é—®é¢˜ã€‚åŸåˆ™ä¸Šï¼Œè¯¥æŠ€æœ¯é€‚ç”¨äºæ— é™é•¿çš„è„‰å†²åºåˆ—ï¼Œè€Œæ— éœ€é€å—å¤„ç†ã€‚</p>

<p>è¿™ç§æ–¹æ³•å’Œä¹‹å‰ä¼ ç»Ÿæ–¹æ³•çš„æ¯”è¾ƒå¦‚ä¸‹å›¾ï¼š
<img src="/assets/img/2022-07-22-review-a-generalized-sampling-method-for-finite-rate-of-innovation-signal-reconstruction/table.png" alt="COMPARISON OF THE STANDARD AND THE PROPOSED APPROACHES" /></p>]]></content><author><name>Yichong Wang</name></author><category term="Paper-Review" /><category term="Signal" /><category term="M.sc" /><summary type="html"><![CDATA[Sampling signals that are not admissible within the classical Shannon framework]]></summary></entry><entry><title type="html">First dive into a NFT public sale and mint</title><link href="https://pinkr1ver.com/art/2022/05/22/first-dive-into-a-nft-public-sale-and-mint.html" rel="alternate" type="text/html" title="First dive into a NFT public sale and mint" /><published>2022-05-22T08:55:00+08:00</published><updated>2022-05-22T08:55:00+08:00</updated><id>https://pinkr1ver.com/art/2022/05/22/first-dive-into-a-nft-public-sale-and-mint</id><content type="html" xml:base="https://pinkr1ver.com/art/2022/05/22/first-dive-into-a-nft-public-sale-and-mint.html"><![CDATA[<h2 id="æˆ‘å¯¹nftçš„çœ‹æ³•åˆå‘ç”Ÿäº†æ”¹å˜">æˆ‘å¯¹NFTçš„çœ‹æ³•åˆå‘ç”Ÿäº†æ”¹å˜</h2>
<p>æˆ‘ä¹‹å‰åœ¨ä¹°æˆ‘äººç”Ÿç¬¬ä¸€ä¸ªNFTï¼Œä¹Ÿå°±æ˜¯<a href="https://rarible.com/token/0x495f947276749ce646f68ac8c248420045cb7b5e:40482595849772694285173713041642282097106100196042549765489076692661152251905?tab=owners">Pixel Giraffe #6759</a>ï¼Œæˆ‘è®¤ä¸ºNFTå°±æ˜¯æ–°æ—¶ä»£çš„QQç§€å˜è£…ï¼Œåªä¸è¿‡å˜æˆäº†éåŒè´¨åŒ–çš„è®¾è®¡ï¼ˆæŠ›å¼€æŠ€æœ¯å±‚é¢æ¥è®²ï¼‰ã€‚ä½†é€šè¿‡å¯¹web3.0çš„æœ€ç®€å•çš„è®¤è¯†ï¼Œè®©æˆ‘æ„è¯†åˆ°NFTçš„åº”ç”¨å½“ç„¶ç»å¯¹ä¸æ˜¯QQç§€å˜è£…é‚£ä¹ˆç®€å•ï¼Œè‡³å°‘ç›®å‰taå…·æœ‰ç»„å»ºç¤¾åŒºï¼Œèº«ä»½è®¤è¯ï¼Œç»Ÿä¸€ç›¸åŒå®¡ç¾çš„äººè¿™ç±»çš„ä½œç”¨ï¼Œå…¶ä¸­å»ä¸­æ€§åŒ–èº«ä»½è®¤è¯æ‰€å¸¦æ¥åç»­çš„å„ç§çš„æœ‰æ„æ€çš„äº‹æƒ…æˆ‘æƒ³è¿˜æœ‰æ›´å¤šå¯èƒ½ã€‚</p>

<p>ä½†åŒæ—¶ï¼Œethé“¾ä¸ŠNFTçƒ­åº¦ä¸‹é™å’Œèµ„é‡‘è½¬ç§»ä¼¼ä¹æ˜¯ç›®å‰ä¸å¯å¦è®¤çš„äº‹å®ï¼Œè¿™åœ¨æˆ‘çœ‹æ¥å¾ˆæœ‰å¯èƒ½å°±æ˜¯NFTæ‰€å¸¦æ¥çš„é‡‘èå±æ€§è®©NFTå‘ç”Ÿäº†æ‰­æ›²ï¼Œå¯èƒ½å…·æœ‰ä¸€å®šä»·å€¼çš„jepgå’Œcodeé€šè¿‡ä¸€ç³»åˆ—çš„æ‰‹æ®µâ€œèµ‹èƒ½â€æœ€ç»ˆè¢«é¡¹ç›®æ–¹æ”¶å‰²åå½’äºè™šæ— ï¼Œæˆ‘ä¹°çš„<a href="https://rarible.com/token/0x495f947276749ce646f68ac8c248420045cb7b5e:40482595849772694285173713041642282097106100196042549765489076692661152251905?tab=owners">Pixel Giraffe #6759</a>å°±æ˜¯è¿™æ ·çš„ä¸€ä¸ªé¡¹ç›®ï¼Œä¸€ä¸ªçº¯çº¯çš„éŸ­èœã€‚ä½†æ˜¯äººå®¶ä¼¼ä¹ä¹Ÿæ²¡æœ‰æä»€ä¹ˆå®£ä¼ èµ‹èƒ½ç¤¾åŒºï¼Œå•çº¯åœ°è´©å–é‡å¤æ’åˆ—ç»„åˆçš„<a href="https://en.wikipedia.org/wiki/Pop_art">æ³¢æ™®è‰ºæœ¯</a>ï¼Œä»æŸç§æ„ä¹‰ä¸Šè®²ï¼Œtaä¹Ÿç®—æ˜¯NFTåˆæœŸæ¢ç´¢çš„çº¯ç²¹ï¼ŸğŸ˜…</p>

<h2 id="boki"><a href="https://www.boki.art/">Boki</a></h2>
<p>æˆ‘åœ¨May 20å·çœ‹åˆ°openseaä¸Šçš„trendingé‡Œæœ‰äº†bokiè¿™ä¸ªé¡¹ç›®ï¼Œbokiç¬¬ä¸€çœ¼çš„æ„Ÿè§‰å°±è®©æˆ‘è§‰å¾—è¯´è¿™ä¸ªé£æ ¼çš„jepgå…·æœ‰<a href="https://www.investopedia.com/terms/b/bluechip.asp">blue chip</a>çš„æ½œåŠ›ï¼Œå¦‚æœå®ƒçš„ç›®çš„çœŸçš„æ˜¯ä¸ºäº†æ¢ç´¢web3.0ï¼Œæˆ‘æ„¿æ„è®©å®ƒæˆä¸ºæˆ‘ç¬¬ä¸€ä¸ªdeep diveçš„NFT communityï¼ˆåŒæ—¶å®ƒéœ€è¦çš„èµ„é‡‘æˆ‘å¯ä»¥è´Ÿæ‹…ğŸ™„ï¼‰ã€‚</p>

<p>äºæ˜¯åœ¨ä¸€æ®µæ—¶é—´çš„ä¿¡æ¯æœé›†å’Œç­‰å¾…ä¸­ï¼Œåœ¨May 20 permintåï¼ŒMay 22å·æ—©æ—©çˆ¬èµ·åºŠï¼Œ7ç‚¹åŠå°±å®ˆåœ¨mint webä¸Šä¸€ç›´F5ï¼ˆå› ä¸ºboki final saleé‡‡ç”¨FCFS raffleï¼Œfirst come first serverï¼‰ï¼Œç„¶å8ï¼š00ï¼Œå‡†æ—¶æ‰“å¼€å°ç‹ç‹¸é’±åŒ…ï¼Œsign contractï¼Œç„¦æ€¥åœ°åœ¨é“¾ä¸Šç­‰å¾…minerï¼Œåœ†åœˆè½¬äº†1min15såï¼Œæˆ‘mintåˆ°äº†<a href="https://rarible.com/token/0x248139afb8d3a2e16154fbe4fb528a3a214fd8e7:5661?tab=details">boki</a>ã€‚</p>

<p><a href="https://www.boki.art/">
    <img src="/assets/img/2022-05-22-first-dive-into-a-nft-public-sale-and-mint/boki.gif" alt="Boki" style="  display: block; margin-left: auto; margin-right: auto;width: 50%;" />
</a></p>

<h2 id="future">Future</h2>
<p>å½“ç„¶ï¼ŒNFTçš„ä»·å€¼å¯èƒ½ä¼šè¿æ¥æš´è·Œï¼Œå°±åƒ20ä¸–çºªåˆçš„<a href="https://en.wikipedia.org/wiki/Dot-com_bubble">Dot-com bubble</a>ï¼Œä½†æ˜¯æˆ–è®¸æœªæ¥åˆçœŸçš„å¯ä»¥<a href="https://kknews.cc/education/o4x9x85.html">bet on crypto</a>å‘¢ï¼Ÿwho knows?</p>

<p>Canâ€™t wait to see the <a href="https://www.boki.art/">Boki</a> art reveal.</p>]]></content><author><name>Yichong Wang</name></author><category term="Art" /><category term="NFT" /><category term="Boki" /><summary type="html"><![CDATA[Boki will go to the moon ğŸš€]]></summary></entry><entry><title type="html">ç¤¼è²Œå¯¹å¾…ä»–äººçš„è¯¢é—®ï¼Œä»¥ç»„ç»‡æ¶æ„ä¸­çš„é€šçŸ¥äººå‘˜ä¸ºä¾‹</title><link href="https://pinkr1ver.com/thinking/2022/04/27/%E7%A4%BC%E8%B2%8C%E5%AF%B9%E5%BE%85%E4%BB%96%E4%BA%BA%E7%9A%84%E8%AF%A2%E9%97%AE-%E4%BB%A5%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E9%80%9A%E7%9F%A5%E4%BA%BA%E5%91%98%E4%B8%BA%E4%BE%8B.html" rel="alternate" type="text/html" title="ç¤¼è²Œå¯¹å¾…ä»–äººçš„è¯¢é—®ï¼Œä»¥ç»„ç»‡æ¶æ„ä¸­çš„é€šçŸ¥äººå‘˜ä¸ºä¾‹" /><published>2022-04-27T15:01:00+08:00</published><updated>2022-04-27T15:01:00+08:00</updated><id>https://pinkr1ver.com/thinking/2022/04/27/%E7%A4%BC%E8%B2%8C%E5%AF%B9%E5%BE%85%E4%BB%96%E4%BA%BA%E7%9A%84%E8%AF%A2%E9%97%AE-%E4%BB%A5%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E9%80%9A%E7%9F%A5%E4%BA%BA%E5%91%98%E4%B8%BA%E4%BE%8B</id><content type="html" xml:base="https://pinkr1ver.com/thinking/2022/04/27/%E7%A4%BC%E8%B2%8C%E5%AF%B9%E5%BE%85%E4%BB%96%E4%BA%BA%E7%9A%84%E8%AF%A2%E9%97%AE-%E4%BB%A5%E7%BB%84%E7%BB%87%E6%9E%B6%E6%9E%84%E4%B8%AD%E7%9A%84%E9%80%9A%E7%9F%A5%E4%BA%BA%E5%91%98%E4%B8%BA%E4%BE%8B.html"><![CDATA[<p>ä»Šå¤©å»é—®å­¦æ ¡è¾…å¯¼å‘˜æœ‰å…³æ¯•ä¸šå°±ä¸å»å‘å¡«å†™çš„é—®é¢˜ï¼Œè®©æˆ‘äº§ç”Ÿäº†ä¸€äº›æ€è€ƒã€‚å½“ä½ å……å½“ä¸€ä¸ªè¢«å’¨è¯¢çš„å¯¹è±¡ï¼Œæˆ–è€…ä½ æ¥å‘æ”¾é€šçŸ¥åæœ‰äººæ¥è¯¢é—®ä½ ï¼Œåˆæˆ–è€…ä½ æ˜¯æŸä¸ªdocçš„ç®¡ç†äººå‘˜ï¼Œè¿™ä¸ªæ—¶å€™åº”è¯¥ä»¥ä»€ä¹ˆæ ·çš„æ€åº¦æ¥å¯¹å¾…è¯¢é—®çš„äººã€‚</p>

<h2 id="ç¤¼è²Œåœ°å¯¹å¾…docé‡Œæœ‰çš„å†…å®¹">ç¤¼è²Œåœ°å¯¹å¾…docé‡Œæœ‰çš„å†…å®¹</h2>
<p>å½“ä½ å‘å‡ºé€šçŸ¥ï¼Œå†™å¥½docï¼Œå¾ˆéš¾è¯´ä¸ä¼šæœ‰ä¸è®¤çœŸçœ‹docçš„äººæ¥é—®ä½ docé‡Œæœ‰çš„å†…å®¹ï¼Œè¿™ä¸ªæ—¶å€™æ‹¿å‡ºè®½åˆºå’Œå†·æ¼ çš„æ€åº¦åœ¨æˆ‘çœ‹æ¥æ˜¯ä¸€ä¸ªé”™è¯¯ã€‚</p>

<p>é¦–å…ˆï¼Œä½ æ‰€ç†è§£çš„çš„äº‹æƒ…å’Œè¯¢é—®äººçš„äº‹æƒ…æ˜¯å¦çœŸçš„ä¸€è‡´å‘¢ï¼Œ<strong>è¿™ä¸€ç‚¹ä»…ä»…åœ¨ä½ çš„ä¸€æ–¹å¾—åˆ°ç¡®è®¤æ˜¯ä¸å¤Ÿçš„</strong>ï¼Œæˆ–è®¸å’¨è¯¢çš„äººè¢«ä½ å‘›å›å»åæ‰¾äº†å¥½ä¹…ä¹Ÿè¿˜æ˜¯æ‰¾ä¸åˆ°ç­”æ¡ˆï¼Œäº‹æƒ…å¾—ä¸åˆ°è§£å†³ï¼Œè¿™ä¸å°±æ˜¯å¤§å®¶æœ€è®¨åŒçš„æµç¨‹å¡ç¯èŠ‚çš„æƒ…å†µå—ï¼Ÿæ—¢ç„¶å¤§å®¶éƒ½è®¨åŒè¿™æ ·ï¼Œå°±ä¸è¦å»åšè¿™æ ·æœ¬æ¥è‡ªå·±ä¹Ÿä¼šè®¨åŒçš„äº‹æƒ…ã€‚</p>

<p>å…¶æ¬¡ï¼Œ<strong>è´£ä»»</strong>ã€‚ä½ æ˜¯ä¸æ˜¯æœ‰å›ç­”è¯¢é—®äººçš„è´£ä»»å‘¢ï¼Œä½œä¸ºä¸€ä¸ªç»„ç»‡æ¶æ„ä¸­å¸®åŠ©ä»–äººçš„è§’è‰²ï¼Œæ¯”å¦‚åœ°æ–¹åŸºå±‚å…¬åŠ¡å‘˜ï¼Œå¤§å­¦è¾…å¯¼å‘˜ï¼Œä½ æ˜¯ä¸æ˜¯æœ‰è´£ä»»æ¥è§£å†³é—®é¢˜å‘¢ï¼Ÿå¦‚æœæœ‰è§£å†³é—®é¢˜çš„éœ€æ±‚ï¼Œä¸€ä¸ª<strong>ç®€å•çš„å›ç­”</strong>ï¼Œä¸€ä¸ªç¤¼è²Œçš„<strong>â€œå¯¹ä¸èµ·ï¼Œæˆ‘ä¸å¤ªæ¸…æ¥šï¼Œæˆ–è®¸è¿™ä¸ªæ–‡ä»¶é‡Œæœ‰ï¼Ÿâ€</strong>æˆ–è®¸ç›¸å¯¹äº â€œä½ æœ‰çœ‹é€šçŸ¥å—ï¼Ÿâ€ å¯¹ä»–äººæ¥è¯´æ›´åŠ æ¸©æš–ã€‚æ›´ä½•å†µå¦‚æœä½ æ˜¯çŸ¥é“ç­”æ¡ˆçš„ï¼Œæ‰“å‡ºè¿™ä¸ªç­”æ¡ˆæˆ‘æƒ³ä¸ä¼šå¾ˆèŠ±æœ¬åº”è¯¥æ˜¯ä½ è´£ä»»ä¸å·¥ä½œçš„æ—¶é—´ã€‚</p>

<p>é¢å¤–ï¼Œå¦‚æœä½ æ˜¯è‡ªæ„¿ç®¡ç†ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæˆ‘æƒ³ä¸€ä¸ªæœ‰ç¤¼è²Œçš„å›å¤ä¹Ÿä¼šç»™ä½ çš„projectå¸¦æ¥æ›´å¥½çš„è§‚æ„Ÿã€‚</p>

<h2 id="ä½œä¸ºä¸ç¾¤ä¼—æ¥è§¦çš„äººå‘˜ä¸è¦æŠŠè‡ªå·±æ”¾åˆ°æ•Œäººçš„ä½ç½®">ä½œä¸ºä¸ç¾¤ä¼—æ¥è§¦çš„äººå‘˜ï¼Œä¸è¦æŠŠè‡ªå·±æ”¾åˆ°æ•Œäººçš„ä½ç½®</h2>
<p>æˆ‘æ—¶å¸¸ä¼šè§‰å¾—è¯´ï¼Œæœ‰äº›æ—¶å€™æœ‰çš„åŸºå±‚å·¥ä½œäººå‘˜å¥½åƒçœŸçš„æ˜¯æŠŠè‡ªå·±æ”¾åˆ°äº†ä»–æ‰€æœåŠ¡çš„ç¾¤ä½“çš„å¯¹ç«‹é¢ã€‚å‘é€šçŸ¥çš„è¯ï¼Œå°±æŠŠé€šçŸ¥ä¸€å‘ï¼Œåˆ«äººæ¥é—®å°±è´£éš¾åˆ«äººæœ‰æ²¡æœ‰çœ‹é€šçŸ¥çš„å‘€ï¼Œ<strong>å¥½åƒæŠŠé€šçŸ¥ä¸€å‘ï¼Œè¿™å¥—å·¥ä½œä¼¼ä¹å°±ç®—å®Œæˆäº†</strong>ï¼Œå…¶ä»–æ¥é—®çš„äººåˆæ€ä¹ˆæ ·å‘¢ã€‚æŠŠé€šçŸ¥ä¸¢ç»™ä»–ä»¬å°±å¥½äº†ã€‚</p>

<p>æˆ‘ä¸ç”±å¾—æƒ³èµ·ä»¥å‰å½“é€šçŸ¥äººå‘˜çš„æ—¶å€™ï¼Œæˆ‘ä¹Ÿæ—¶å¸¸çŠ¯è¿™æ ·çš„é”™è¯¯ã€‚</p>

<p>æ›´ä¸è¦å»åšä¸å’¨è¯¢ä½ çš„äººè¾©è®ºçš„äº‹æƒ…ï¼Œ<strong>ä¸è¦å› ä¸ºåˆ©ç”¨ä¿¡æ¯å·®å»å¯¹é—®ä½ çš„äººå’„å’„é€¼äºº</strong>ï¼Œ æ›´ä½•å†µä½ è¿˜æ˜¯åœ¨ä¸€ä¸ªæœåŠ¡å²—ä½ä¸Šçš„æ—¶å€™ã€‚<strong>ä¿¡æ¯çš„ä¸¢å¤±æ˜¯ä¼šäººæ„Ÿåˆ°æƒ¶æçš„</strong>ï¼Œæœ¬åº”è¯¥æ˜¯æ•‘ç”Ÿå‘˜çš„ä½ ï¼Œä¸è¦å»å½“é²¨é±¼ã€‚</p>

<h2 id="conclusion">Conclusion</h2>
<p>è¿™æ ·çš„äººï¼Œç›¸ä¿¡è·Ÿæˆ‘æœ‰ç€åŒæ ·ç»å†çš„äººæˆ–å¤šæˆ–å°‘éƒ½ç»å†ç€ï¼Œå› ä¸ºæˆ‘ç”Ÿæ´»åœ¨ä¸­å›½ï¼Œæ‰€ä»¥æˆ‘çŸ¥é“è¿™æ ·çš„åŸºå±‚å·¥ä½œäººå‘˜éå¸ƒä¸­å›½çš„å¤§è¡—å°å··ã€‚æˆ‘ä¸çŸ¥é“é€ æˆçš„åŸå› æ˜¯ä»€ä¹ˆï¼Œä½†æ˜¯å¸Œæœ›å½“å¤§å®¶æ‰‹é‡Œæœ‰ç€æƒåŠ›çš„æ—¶å€™ï¼Œå“ªæ€•æ˜¯å†å°çš„æƒåˆ©ï¼Œéƒ½å¯ä»¥<strong>æ¸©æŸ”ä¸€ç‚¹</strong>ã€‚</p>]]></content><author><name>Yichong Wang</name></author><category term="Thinking" /><category term="thinking" /><category term="life" /><summary type="html"><![CDATA[Be polite and patience is a key to be a good communicator]]></summary></entry><entry><title type="html">Commemorate 2022 Pixel War</title><link href="https://pinkr1ver.com/art/2022/04/06/commemorate-2022-pixel-war.html" rel="alternate" type="text/html" title="Commemorate 2022 Pixel War" /><published>2022-04-06T00:00:00+08:00</published><updated>2022-04-06T00:00:00+08:00</updated><id>https://pinkr1ver.com/art/2022/04/06/commemorate-2022-pixel-war</id><content type="html" xml:base="https://pinkr1ver.com/art/2022/04/06/commemorate-2022-pixel-war.html"><![CDATA[<p><img src="/assets/img/2022-04-06-commemorate-2022-pixel-war/r_place_2022.png" alt="r/place 2022" /></p>

<p>The afterglow of the Internet age</p>]]></content><author><name>Yichong Wang</name></author><category term="Art" /><category term="pixel art" /><category term="reddit" /><summary type="html"><![CDATA[r/place 2022]]></summary></entry><entry><title type="html">Data type is medical image normalization trap</title><link href="https://pinkr1ver.com/bme/2022/04/06/data-type-is-medical-image-normalization-trap.html" rel="alternate" type="text/html" title="Data type is medical image normalization trap" /><published>2022-04-06T00:00:00+08:00</published><updated>2022-04-06T00:00:00+08:00</updated><id>https://pinkr1ver.com/bme/2022/04/06/data-type-is-medical-image-normalization-trap</id><content type="html" xml:base="https://pinkr1ver.com/bme/2022/04/06/data-type-is-medical-image-normalization-trap.html"><![CDATA[<h1 id="background">Background</h1>

<p>I want to do preprocessing to some MRI data with .nii file extension. One of the step is normalization. As usual, when our dataset is a bunch of normal images with .png file extension, its usually a 3 Channel 8-bit image with data type is uint8. So its very common to do normalization from [0-225] to [0-1] by just dividing 255 to very pixel value.</p>

<p>But my dataset is all float64 aka. double type.</p>

<h1 id="wrong-operation">Wrong operation</h1>

<p>When I first do data preprocessing, I make two fatally error.</p>
<ul>
  <li>Convert .nii file every slice into uint8, typical normal image in computer.</li>
  <li>Do normalization to every slice</li>
</ul>

<p>First fatal is because that I do a real lossy conversation from float64 to uint8.</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">imageio</span><span class="p">.</span><span class="n">imwrite</span><span class="p">(</span><span class="n">os</span><span class="p">.</span><span class="n">path</span><span class="p">.</span><span class="n">join</span><span class="p">(</span><span class="n">savePathAX</span><span class="p">,</span> <span class="s">'{}.png'</span><span class="p">.</span><span class="nb">format</span><span class="p">(</span><span class="n">i</span><span class="p">)),</span> <span class="nb">slice</span><span class="p">)</span>
</code></pre></div></div>
<p>This step will auto convert your data to uint8, its a real lossy conversation.</p>

<p>Also, you may have a question, float64 range from 2.2E-308 to 1.7E+308, how do this code deal with the largest value.</p>

<p>Actually, it will make your largest value as 255. So it means that, it already do the normalization to every image from [0 - max] to [0 - 255].</p>

<p>Its the second fatally error.</p>

<p>For medical image, take CT as example:</p>

<p><a href="https://zhuanlan.zhihu.com/p/112176670"><img src="/assets/img/2022-04-06-data-type-is-medical-image-normalization-trap/Zhihu.jpg" alt="CT Subtance HU value" /></a></p>

<p>You can find the pixel value mean, so you can find the data max bound and min bound[1].  And you can do the normalization easily:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">MIN_BOUND</span> <span class="o">=</span> <span class="o">-</span><span class="mf">1000.0</span>
<span class="n">MAX_BOUND</span> <span class="o">=</span> <span class="mf">400.0</span>

<span class="k">def</span> <span class="nf">norm_img</span><span class="p">(</span><span class="n">image</span><span class="p">):</span> 
    <span class="n">image</span> <span class="o">=</span> <span class="p">(</span><span class="n">image</span> <span class="o">-</span> <span class="n">MIN_BOUND</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">MAX_BOUND</span> <span class="o">-</span> <span class="n">MIN_BOUND</span><span class="p">)</span>
    <span class="n">image</span><span class="p">[</span><span class="n">image</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
    <span class="n">image</span><span class="p">[</span><span class="n">image</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="k">return</span> <span class="n">image</span>
</code></pre></div></div>
<p>(This code also from [1])</p>

<p>So, image that you normalize every image into [0-1], if your every image have the bone, thatâ€™s fine. Every max value 1 represent bone. But if one of your image donâ€™t have bone, the max HU value represent soft tissue and you normalize it into [0-1]. So different imageâ€™s 1 will have different mean, normally it means bone and some image will wrongly represent it as soft issue.</p>

<p>Thatâ€™s why you have normalize depending on the meaning of its pixel value rather than just every image or data type max value.</p>

<p>Take my MRI image dataset as example, its max value is 2.6E+4 and its type is float64. If you divide it by float_max aka. 1.7E+308, all its value will be super small and if you transfer it into float32 to input into model, all its pixel value will be 0.</p>

<p>Lastly, in conclusion,</p>

<ul>
  <li>Find the pixel value mean before do normalization</li>
  <li>Data type is not the guideline and most of time using float64 is for its precision rather than its range.</li>
</ul>

<h3 id="appdenix">Appdenix</h3>
<p>[1] https://zhuanlan.zhihu.com/p/112176670</p>

<p><del><em>For MRI Image, i donâ€™t know there has the standard or not.</em></del></p>

<h1 id="replenish-apr-8-2022">Replenish Apr 8, 2022</h1>
<p>In MRI, you can do normalization to every brain because in MRI, different machine, different machine parameter will give you real different intensity. So the absolute value of MRI image intensity is not every important, the most important value is contrast information, aka. the intensity histogram. So you can do normalization to every brain separately.</p>

<p>â€œThe same histogram can maintain the internal tissue contrast of your original individual and reduce the gray value difference between individualsâ€ Â Â Â Â Â Â       â€”Shen (A researcher from Zhejiang University)</p>]]></content><author><name>Yichong Wang</name></author><category term="BME" /><category term="Deep learning" /><category term="MRI" /><category term="BME" /><summary type="html"><![CDATA[data type is not always the normalization standard]]></summary></entry><entry><title type="html">G-mean is not suitable for medical image instance segmentation to find threshold when background is too much</title><link href="https://pinkr1ver.com/bme/2022/04/03/g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much.html" rel="alternate" type="text/html" title="G-mean is not suitable for medical image instance segmentation to find threshold when background is too much" /><published>2022-04-03T00:00:00+08:00</published><updated>2022-04-03T00:00:00+08:00</updated><id>https://pinkr1ver.com/bme/2022/04/03/g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much</id><content type="html" xml:base="https://pinkr1ver.com/bme/2022/04/03/g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much.html"><![CDATA[<h1 id="threshold">Threshold</h1>
<p>In my undergraduate FYP, I need to do image segmentation to the brain MRI to annotate the GBM cancer in patient brain.</p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/GBM_MRI.png" alt="GBM MRI Image" /></p>

<p>I apply the famous architecture U-net to do this segmentation task. The last layer I apply is <strong>sigmoid</strong>. It outputs every pixel value in [0-1]. I was wondering which value is the best threshold for the segmentation.
<img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/Threshold.png" alt="Threshold" />
In the beginning, I set threshold as 0.5, but the data is very imbalance that the background is much more than the cancer area. It means that 0 value pixel is much more that 1 value pixel. It will lead to 0.5 is not a suitable value to distinguish 1 and 0. You can check this link to learn the detail:</p>

<p>https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293</p>

<h1 id="roc-curve">ROC curve</h1>
<p>ROC (Receiver operating characteristic), is a graphical plot that illustrates the diagnostic ability of a binary classifier system as its discrimination threshold is varied. It is created by plotting the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings.</p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/Roccurves.png" alt="ROC Curve" /></p>

<p>It has two usages here:</p>
<ol>
  <li>Compare different method in binary classification.</li>
  <li>Find best threshold for binary classification</li>
</ol>

<p>For the usage 1, it is always using AUC (Area under curve) to determine which classification method is better.</p>

<p>And for usage 2, which is the part I want to talk about, it always want to choose the most up left point to be our threshold. It is very easy to understand because the more left, the false positive rate will be more less,  the more up, the true positive rate will be more bigger. But how to evaluate the best left up point in ROC curve? As the article in <a href="https://towardsdatascience.com/optimal-threshold-for-imbalanced-classification-5884e870c293">link above</a> shiows that, it usually use a paramter called G-mean:</p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/Gmean.png" alt="G-mean" /></p>

<p>You can see that G-mean take both sensitivity and specificity into consideration. The result will be a tradeoff for 0 and 1 classification. The point here is that G-mean take 0 and 1 as the same importance. It will be great if we classify banana and apple, or some other two thing in a same significance. It will solve the imbalance.</p>

<p>But the question is that, in medical segmentation to segment cancer area. We do not think cancer area and normal area as same importance. So using  G-mean to determine threshold will result:</p>

<p><strong><em>I do the ROC calculating to find the bigger G-mean point to determine threshold in epoch 10, and you can see the problem.</em></strong></p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/RealRoc.png" alt="Roc curve" /></p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/epoch2_epoch12_specificity.png" alt="Specificity" /></p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/epoch2_epoch12_sensitivity.png" alt="Sensitivity" />
As you can see, to make G-mean bigger, we choose the threshold that will sacrifice specificity to increase sensitivity. Because in my dataset, the background is so much, so specificity will be so close to 1. Even you predict more cancer area, aka. more white part, the specificity will be influenced very little.</p>

<p>As you can see, the specificity decrease from 1 to 0.94, but the sensitivity increase from 0.65 - 1.</p>

<p>To be more clear, see the graph:</p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/1025.png" alt="example" />
The left is original image, the middle is ground truth, the right is prediction.</p>

<p>As you can see, the net wants to predict more white part to increase sensitivity and because it have a lot of picture with huge black part, the specificity will not be influenced more. So finally, choose the biggest G-mean point will lead to:</p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/epoch2_epoch12_f1score.png" alt="f1score" /></p>

<p><img src="/assets/img/2022-04-03-g-mean-is-not-suitable-for-medical-image-instance-segmentation-to-find-threshold-when-background-is-too-much/epoch2_epoch12_IoU.png" alt="IoU" /></p>

<p>The most important parameter we take attention to: f1-score and IoU will dramatically decrease.</p>

<p>In conclusion, when you meet this situation:</p>

<ul>
  <li>Imbalance Data</li>
  <li>You take more attention and account to the small part type of your dataset. (The Brain cancer segmentation in MRI is this typical example )</li>
</ul>

<p>You have better not use G-mean to determine which threshold you shold deploy.</p>]]></content><author><name>Yichong Wang</name></author><category term="BME" /><category term="Deep learning" /><category term="Image Segmentation" /><category term="Threshold" /><category term="Python" /><summary type="html"><![CDATA[Is G-mean can handle medical image instance, especially for small cancer in big brain?]]></summary></entry><entry><title type="html">I buy my first NFT avatar</title><link href="https://pinkr1ver.com/art/2022/03/27/i-buy-my-first-nft-avatar.html" rel="alternate" type="text/html" title="I buy my first NFT avatar" /><published>2022-03-27T00:00:00+08:00</published><updated>2022-03-27T00:00:00+08:00</updated><id>https://pinkr1ver.com/art/2022/03/27/i-buy-my-first-nft-avatar</id><content type="html" xml:base="https://pinkr1ver.com/art/2022/03/27/i-buy-my-first-nft-avatar.html"><![CDATA[<h1 id="story">Story</h1>
<p>Because Boring.</p>

<p>Because in China, cryptocurrency is totally illegal. Even I am in Singapore right now, buy ETH from some <em>Centralized Exchanges</em> is totally impossible. Another way to buy ETH is <em>Decentralized exchanges (DEXs)</em>. It needs a lot of money to do this p2p exchange and I can not afford it. Luckily, I find a way to exchange BTC to ETH considering I have some BTC in hand (from <em>Zhu Yicheng</em>), using <a href="https://changelly.com/">changelly</a>.</p>

<p>Very interesting story, I find a NFT avatar have Hawaiian shirt and knitted hat which are two thing can represent me in the period of Singapore. So I think is it a real serendipity to meet it. So i decided to buy it. It is like:</p>

<h2 id="nft-giraffe-6759-from-collection-pixel-giraffes">NFT: Giraffe #6759 from collection <a href="https://opensea.io/collection/pixel-giraffes">Pixel Giraffes</a></h2>
<p><a href="https://opensea.io/assets/0x495f947276749ce646f68ac8c248420045cb7b5e/40482595849772694285173713041642282097106100196042549765489076692661152251905"><img src="/assets/img/2022-03-27-i-buy-my-first-nft-avatar/Giraffe6759.png" alt="Giraffe #6759" /></a></p>

<h2 id="at-last">At last</h2>

<p>Finally, in my opinion, I think NFT is a great way to create art and sell art. But the most NFT avatar is just a new stupid â€˜æ¢è£…å°æ¸¸æˆâ€™ or new â€˜QQç§€â€™ that you can choose some elements provided by others to change your web avatar. It is not web3, just a new way to take your money out like 10 years ago.</p>]]></content><author><name>Yichong Wang</name></author><category term="Art" /><category term="NFT" /><category term="avatar" /><summary type="html"><![CDATA[My first NFT]]></summary></entry><entry><title type="html">ChinaPunkğŸ˜‚</title><link href="https://pinkr1ver.com/art/2022/03/26/chinapunk.html" rel="alternate" type="text/html" title="ChinaPunkğŸ˜‚" /><published>2022-03-26T00:00:00+08:00</published><updated>2022-03-26T00:00:00+08:00</updated><id>https://pinkr1ver.com/art/2022/03/26/chinapunk</id><content type="html" xml:base="https://pinkr1ver.com/art/2022/03/26/chinapunk.html"><![CDATA[<h1 id="china-punk">China punk</h1>
<p><a href="https://twitter.com/damienics/status/1506354996413091847"><img src="/assets/img/2022-03-26-chinapunk/Chinapunk.jpg" alt="ChinaPunk" /></a>
From: <a href="https://twitter.com/damienics/status/1506354996413091847"><del>Damien Ma</del></a> <a href="https://twitter.com/hancocktom">Tom Hancock</a></p>]]></content><author><name>Yichong Wang</name></author><category term="Art" /><category term="Photo" /><summary type="html"><![CDATA[China style cyberpunk in 2022. ğŸ˜‚]]></summary></entry><entry><title type="html">In python cv2, you need use .copy() method in Numpy to create a copy</title><link href="https://pinkr1ver.com/python/2022/03/25/in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy.html" rel="alternate" type="text/html" title="In python cv2, you need use .copy() method in Numpy to create a copy" /><published>2022-03-25T00:00:00+08:00</published><updated>2022-03-25T00:00:00+08:00</updated><id>https://pinkr1ver.com/python/2022/03/25/in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy</id><content type="html" xml:base="https://pinkr1ver.com/python/2022/03/25/in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy.html"><![CDATA[<p>Recently, I need to upload my graduation photo in China. There are lots of requirements, so I need to do some processing to my picture. One of them is to do offset, moving the picture pixel value down.</p>

<p>Because I donâ€™t very familiar with cv2 function, so I decide to do brute force way, using <code class="language-plaintext highlighter-rouge">for loop</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_img</span> <span class="o">=</span> <span class="n">img</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">new_img</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">20</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">img</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
        <span class="n">new_img</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">]</span>
</code></pre></div></div>

<p>But with this code, the result will be wrong like that:</p>
<h2 id="original-image">Original Image:</h2>
<p><img src="/assets/img/2022-03-25-in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy/images.png" alt="" /></p>

<h2 id="wrong-results">Wrong Results:</h2>
<p><img src="/assets/img/2022-03-25-in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy/NewImage.jpg" alt="" /></p>

<p>If you use another method to do this:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_img</span> <span class="o">=</span> <span class="n">img</span><span class="p">[:</span><span class="mi">235</span><span class="p">,:]</span>
<span class="n">bak</span> <span class="o">=</span> <span class="n">img</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">20</span><span class="p">,:]</span>


<span class="n">new_img</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">bak</span><span class="p">,</span> <span class="n">new_img</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>


<span class="n">plt</span><span class="p">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>

</code></pre></div></div>
<h2 id="expected-results">Expected Results:</h2>
<p><img src="/assets/img/2022-03-25-in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy/NewImage2.jpg" alt="" /></p>

<p>The reason why is that when you code <code class="language-plaintext highlighter-rouge">new_img = img</code> in python. The numpy will not create a real variable with new address. See this code:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">255</span><span class="p">,</span> <span class="mi">255</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">x</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="nb">id</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="o">&gt;</span> <span class="mi">2410142026512</span>
<span class="o">&gt;</span> <span class="mi">2410142026512</span>

</code></pre></div></div>
<p>Guess what, the result is that address of x and y is the same. I guess that when numpy has some calculation will alert numpy to create a real variable. If there is just â€˜=â€™, the numpy will not create real variable. So you need to use .copy() function to create copy of one variable, rather than â€˜=â€™ .</p>

<p>I also ask this question in <a href="https://stackoverflow.com/questions/71606098/python-opencv2-for-loop-to-change-image-pixel-value?noredirect=1#comment126563050_71606098">stackoverflow</a>. This answer is so great. It introduce the concept about <strong>shallow copy and deep copy</strong>.</p>

<p><img src="/assets/img/2022-03-25-in-python-cv2-you-need-use-copy-method-in-numpy-to-create-a-copy/ShallowCopy&amp;DeepCopy.png" alt="" /></p>]]></content><author><name>Yichong Wang</name></author><category term="Python" /><category term="Python" /><category term="opencv-python" /><summary type="html"><![CDATA[In python cv2, you need use .copy() method in Numpy to create a copy]]></summary></entry><entry><title type="html">What is .nii file?</title><link href="https://pinkr1ver.com/bme/2022/03/08/what-is-nii-file.html" rel="alternate" type="text/html" title="What is .nii file?" /><published>2022-03-08T00:00:00+08:00</published><updated>2022-03-08T00:00:00+08:00</updated><id>https://pinkr1ver.com/bme/2022/03/08/what-is-nii-file</id><content type="html" xml:base="https://pinkr1ver.com/bme/2022/03/08/what-is-nii-file.html"><![CDATA[<h1 id="what-is-nii-file">What is .nii file?</h1>

<p>As a BME student, I always meet .nii file though my third and fourth year in college. But I donâ€™t understand .nii file for a long while. So I want to explain the .nii file format very simply to everyone majored in BME.</p>

<h2 id="what-is-nifti">What is NIfTI?</h2>
<p>The first thing we need to know is the background of .nii file â€“ NIfTI (Neuroimaing Informatics Technology Initiative). NIfTI file format was envisioned about a decade ago as a replacement to the then widespread, yet problematic, analyze 7.5 file format.[1] The main problem of previous file format is lacking adequate information about orientation in space. The primary goal of NIfTI is to provide coordinated and targeted service, training, and research to speed the development and enhance the utility of informatics tools related to neuroimaging.[2]</p>

<h2 id="nifti-1--nifti-2">NIfTI-1 &amp; NIfTI-2</h2>
<p>The new format called NIfTI-1, which was defined in two meetings of the so called Data Format Working Group(DFWG) and the National Insitutes of Health(NIH), one in the 31 March and another in 02 September of 2003[1]. NIfTI-2 improves the data types supported by NIfTI-1, as well as precision and voxel size[3].
Both .nii and .nii.gz are the common file extension name for NIfTI file format and commonly, the are both acceptable for most of software. (.nii,gz is .nii compression file)</p>

<h2 id="detail-data-information">Detail Data Information</h2>
<p>We can learn NIfTI file by reading it metadata[4], you can also useing</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>info = niftiinfo(filename)
</code></pre></div></div>
<p>in MATLAB to read meta data from .nii file. 
You can see:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ans = struct with fields:
        sizeof_hdr: 348
          dim_info: ' '
               dim: [3 256 256 21 1 1 1 1]
         intent_p1: 0
         intent_p2: 0
         intent_p3: 0
       intent_code: 0
          datatype: 2
            bitpix: 8
       slice_start: 0
            pixdim: [1 1 1 1 0 0 0 0]
        vox_offset: 352
         scl_slope: 0
         scl_inter: 0
         slice_end: 0
        slice_code: 0
        xyzt_units: 0
           cal_max: 0
           cal_min: 0
    slice_duration: 0
           toffset: 0
           descrip: ''
          aux_file: ''
        qform_code: 0
        sform_code: 0
         quatern_b: 0
         quatern_c: 0
         quatern_d: 0
         qoffset_x: 0
         qoffset_y: 0
         qoffset_z: 0
            srow_x: [0 0 0 0]
            srow_y: [0 0 0 0]
            srow_z: [0 0 0 0]
       intent_name: ''
             magic: 'n+1 '
</code></pre></div></div>
<p>These metadata all stored in the header, which you cann look up offical doc <a href="https://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h">nifti1.h</a> to see how this metadata store. 
And you can use <a href="https://github.com/sharkdp/hexyl">hexyl</a> to show hex code of header info.</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hexyl -n 348 filenaem.nii
</code></pre></div></div>
<p>There are some important header info to show how NIfTI file works.</p>

<h3>â€¦</h3>

<h2 id="appendix-and-reference">Appendix and Reference</h2>
<p>[1] https://brainder.org/2012/09/23/the-nifti-file-format/</p>

<p>[2] https://nifti.nimh.nih.gov/</p>

<p>[3] https://docs.safe.com/fme/html/FME_Desktop_Documentation/FME_ReadersWriters/nifti/nifti.htm</p>

<p>[4] https://nifti.nimh.nih.gov/pub/dist/src/niftilib/nifti1.h</p>]]></content><author><name>Yichong Wang</name></author><category term="BME" /><category term="BME" /><category term="MRI" /><summary type="html"><![CDATA[As a BME student, I always meet .nii file though my third and fourth year in college. But I don't understand .nii file for a long while. So I want to explain the .nii file format very simply to everyone majored in BME.]]></summary></entry></feed>